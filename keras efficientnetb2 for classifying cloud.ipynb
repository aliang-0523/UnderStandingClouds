{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from keras.applications.densenet import DenseNet201\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import Sequence\n",
    "from albumentations import Compose, VerticalFlip, HorizontalFlip, Rotate, GridDistortion\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from numpy.random import seed\n",
    "seed(10)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_folder = '../understandingclouds_data/test_images/'\n",
    "train_imgs_folder = '../understandingclouds_data/train_images/'\n",
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "train_df = pd.read_csv('../understandingclouds_data/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将原始数据转化为列为[image,class,fish,flower,suger,gravel]的dataFrame\n",
    "train_df = train_df[~train_df['EncodedPixels'].isnull()]\n",
    "train_df['Image'] = train_df['Image_Label'].map(lambda x: x.split('_')[0])\n",
    "train_df['Class'] = train_df['Image_Label'].map(lambda x: x.split('_')[1])\n",
    "classes = train_df['Class'].unique()\n",
    "train_df = train_df.groupby('Image')['Class'].agg(set).reset_index()\n",
    "for class_name in classes:\n",
    "    train_df[class_name] = train_df['Class'].map(lambda x: 1 if class_name in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将图片和标签转化为字典\n",
    "img_2_ohe_vector = {img:vec for img, vec in zip(train_df['Image'], train_df.iloc[:, 2:].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将图片划分为训练集以及测试集\n",
    "train_imgs, val_imgs = train_test_split(train_df['Image'].values,\n",
    "                                        test_size=0.2,\n",
    "                                        stratify=train_df['Class'].map(lambda x: str(sorted(list(x)))),\n",
    "                                        random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs[:10],val_imgs[:10],train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenenerator(Sequence):\n",
    "    def __init__(self, images_list=None, folder_imgs=train_imgs_folder, \n",
    "                 batch_size=32, shuffle=True, augmentation=None,\n",
    "                 resized_height=260, resized_width=260, num_channels=3):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentation = augmentation\n",
    "        if images_list is None:\n",
    "            self.images_list = os.listdir(folder_imgs)\n",
    "        else:\n",
    "            self.images_list = deepcopy(images_list)\n",
    "        self.folder_imgs = folder_imgs\n",
    "        self.len = len(self.images_list) // self.batch_size\n",
    "        self.resized_height = resized_height\n",
    "        self.resized_width = resized_width\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = 4\n",
    "        self.is_test = not 'train' in folder_imgs\n",
    "        if not shuffle and not self.is_test:\n",
    "            self.labels = [img_2_ohe_vector[img] for img in self.images_list[:self.len*self.batch_size]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_batch = self.images_list[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        X = np.empty((self.batch_size, self.resized_height, self.resized_width, self.num_channels))\n",
    "        y = np.empty((self.batch_size, self.num_classes))\n",
    "\n",
    "        for i, image_name in enumerate(current_batch):\n",
    "            path = os.path.join(self.folder_imgs, image_name)\n",
    "            img = cv2.resize(cv2.imread(path), (self.resized_height, self.resized_width)).astype(np.float32)\n",
    "            if not self.augmentation is None:\n",
    "                augmented = self.augmentation(image=img)\n",
    "                img = augmented['image']\n",
    "            X[i, :, :, :] = img/255.0\n",
    "            if not self.is_test:\n",
    "                y[i, :] = img_2_ohe_vector[image_name]\n",
    "        return X, y\n",
    "\n",
    "    def get_labels(self):\n",
    "        if self.shuffle:\n",
    "            images_current = self.images_list[:self.len*self.batch_size]\n",
    "            labels = [img_2_ohe_vector[img] for img in images_current]\n",
    "        else:\n",
    "            labels = self.labels\n",
    "        return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumentations_train = Compose([\n",
    "    VerticalFlip(), HorizontalFlip(), Rotate(limit=20), GridDistortion()\n",
    "], p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_train = DataGenenerator(train_imgs, augmentation=albumentations_train)\n",
    "data_generator_train_eval = DataGenenerator(train_imgs, shuffle=False)\n",
    "data_generator_val = DataGenenerator(val_imgs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data_generator_train.get_labels()#[fish,flower,sugar,gravel]one-hot之后的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrAucCallback(Callback):\n",
    "    def __init__(self, data_generator, num_workers=num_cores, \n",
    "                 early_stopping_patience=5, \n",
    "                 plateau_patience=3, reduction_rate=0.5,\n",
    "                 stage='train', checkpoints_path='checkpoints/'):\n",
    "        super(Callback, self).__init__()\n",
    "        self.data_generator = data_generator\n",
    "        self.num_workers = num_workers\n",
    "        self.class_names = ['Fish', 'Flower', 'Sugar', 'Gravel']\n",
    "        #观察history形状\n",
    "        self.history = [[] for _ in range(len(self.class_names) + 1)] # to store per each class and also mean PR AUC\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.plateau_patience = plateau_patience\n",
    "        self.reduction_rate = reduction_rate\n",
    "        self.stage = stage\n",
    "        #初始化最好的为负无穷\n",
    "        self.best_pr_auc = -float('inf')\n",
    "        if not os.path.exists(checkpoints_path):\n",
    "            os.makedirs(checkpoints_path)\n",
    "        self.checkpoints_path = checkpoints_path\n",
    "        \n",
    "    def compute_pr_auc(self, y_true, y_pred):\n",
    "        pr_auc_mean = 0\n",
    "        print(f\"\\n{'#'*30}\\n\")\n",
    "        for class_i in range(len(self.class_names)):\n",
    "            precision, recall, _ = precision_recall_curve(y_true[:, class_i], y_pred[:, class_i])\n",
    "            print('recal:{recall},precision:{precision}'.format(recall[:10],precision[:10]))\n",
    "            pr_auc = auc(recall, precision)\n",
    "            pr_auc_mean += pr_auc/len(self.class_names)\n",
    "            print(f\"PR AUC {self.class_names[class_i]}, {self.stage}: {pr_auc:.3f}\\n\")\n",
    "            self.history[class_i].append(pr_auc)        \n",
    "        print(f\"\\n{'#'*20}\\n PR AUC mean, {self.stage}: {pr_auc_mean:.3f}\\n{'#'*20}\\n\")\n",
    "        self.history[-1].append(pr_auc_mean)\n",
    "        return pr_auc_mean\n",
    "              \n",
    "    def is_patience_lost(self, patience):\n",
    "        if len(self.history[-1]) > patience:\n",
    "            best_performance = max(self.history[-1][-(patience + 1):-1])\n",
    "            return best_performance == self.history[-1][-(patience + 1)] and best_performance >= self.history[-1][-1]    \n",
    "              \n",
    "    def early_stopping_check(self, pr_auc_mean):\n",
    "        if self.is_patience_lost(self.early_stopping_patience):\n",
    "            self.model.stop_training = True    \n",
    "              \n",
    "    def model_checkpoint(self, pr_auc_mean, epoch):\n",
    "        if pr_auc_mean > self.best_pr_auc:\n",
    "            # remove previous checkpoints to save space\n",
    "            for checkpoint in glob.glob(os.path.join(self.checkpoints_path, 'classifier_densenet169_epoch_*')):\n",
    "                os.remove(checkpoint)\n",
    "            self.best_pr_auc = pr_auc_mean\n",
    "            self.model.save(os.path.join(self.checkpoints_path, f'classifier_densenet169_epoch_{epoch}_val_pr_auc_{pr_auc_mean}.h5'))              \n",
    "            print(f\"\\n{'#'*20}\\nSaved new checkpoint\\n{'#'*20}\\n\")\n",
    "              \n",
    "    def reduce_lr_on_plateau(self):\n",
    "        if self.is_patience_lost(self.plateau_patience):\n",
    "            new_lr = float(keras.backend.get_value(self.model.optimizer.lr)) * self.reduction_rate\n",
    "            keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "            print(f\"\\n{'#'*20}\\nReduced learning rate to {new_lr}.\\n{'#'*20}\\n\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict_generator(self.data_generator, workers=self.num_workers)\n",
    "        y_true = self.data_generator.get_labels()\n",
    "        # estimate AUC under precision recall curve for each class\n",
    "        pr_auc_mean = self.compute_pr_auc(y_true, y_pred)\n",
    "              \n",
    "        if self.stage == 'val':\n",
    "            # early stop after early_stopping_patience=4 epochs of no improvement in mean PR AUC\n",
    "            self.early_stopping_check(pr_auc_mean)\n",
    "\n",
    "            # save a model with the best PR AUC in validation\n",
    "            self.model_checkpoint(pr_auc_mean, epoch)\n",
    "\n",
    "            # reduce learning rate on PR AUC plateau\n",
    "            self.reduce_lr_on_plateau()            \n",
    "        \n",
    "    def get_pr_auc_history(self):\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric_callback = PrAucCallback(data_generator_train_eval)\n",
    "val_callback = PrAucCallback(data_generator_val, stage='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.keras as efn\n",
    "def get_model():\n",
    "    K.clear_session()\n",
    "    base_model =  efn.EfficientNetB2(weights='imagenet', include_top=False, pooling='avg', input_shape=(260, 260, 3))\n",
    "    x = base_model.output\n",
    "    y_pred = Dense(4, activation='sigmoid')(x)\n",
    "    return Model(inputs=base_model.input, outputs=y_pred)\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_layer in model.layers[:-3]:\n",
    "    base_layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-5), loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history_0 = model.fit_generator(generator=data_generator_train,\n",
    "                                validation_data=data_generator_val,\n",
    "                                epochs=20,\n",
    "                                callbacks=[train_metric_callback, val_callback],\n",
    "                                workers=num_cores,\n",
    "                                verbose=1\n",
    "                                )\n",
    "for base_layer in model.layers[:-3]:\n",
    "    base_layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-5), loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history_1 = model.fit_generator(generator=data_generator_train,\n",
    "                                validation_data=data_generator_val,\n",
    "                                epochs=20,\n",
    "                                callbacks=[train_metric_callback, val_callback],\n",
    "                                workers=num_cores,\n",
    "                                verbose=1,\n",
    "                                initial_epoch=1\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_dots(ax,np_array):\n",
    "    ax.scatter(list(range(1, len(np_array) + 1)), np_array, s=50)\n",
    "    ax.plot(list(range(1, len(np_array) + 1)), np_array)\n",
    "pr_auc_history_train = train_metric_callback.get_pr_auc_history()\n",
    "pr_auc_history_val = val_callback.get_pr_auc_history()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_with_dots(plt, pr_auc_history_train[-1])\n",
    "plot_with_dots(plt, pr_auc_history_val[-1])\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Mean PR AUC', fontsize=15)\n",
    "plt.legend(['Train', 'Val'])\n",
    "plt.title('Training and Validation PR AUC', fontsize=20)\n",
    "plt.savefig('pr_auc_hist.png')\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_with_dots(plt, history_0.history['loss']+history_1.history['loss'])\n",
    "plot_with_dots(plt, history_0.history['val_loss']+history_1.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Binary Crossentropy', fontsize=15)\n",
    "plt.legend(['Train', 'Val'])\n",
    "plt.title('Training and Validation Loss', fontsize=20)\n",
    "plt.savefig('loss_hist.png')\n",
    "class_names = ['Fish', 'Flower', 'Sugar', 'Gravel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_for_recall(y_true, y_pred, class_i, recall_threshold=0.94, precision_threshold=0.90, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true[:, class_i], y_pred[:, class_i])\n",
    "    i = len(thresholds) - 1\n",
    "    best_recall_threshold = None\n",
    "    while best_recall_threshold is None:\n",
    "        next_threshold = thresholds[i]\n",
    "        next_recall = recall[i]\n",
    "        if next_recall >= recall_threshold:\n",
    "            best_recall_threshold = next_threshold\n",
    "        i -= 1\n",
    "\n",
    "    # consice, even though unnecessary passing through all the values\n",
    "    best_precision_threshold = [thres for prec, thres in zip(precision, thresholds) if prec >= precision_threshold][0]\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.step(recall, precision, color='r', alpha=0.3, where='post')\n",
    "        plt.fill_between(recall, precision, alpha=0.3, color='r')\n",
    "        plt.axhline(y=precision[i + 1])\n",
    "        recall_for_prec_thres = [rec for rec, thres in zip(recall, thresholds)\n",
    "                                 if thres == best_precision_threshold][0]\n",
    "        plt.axvline(x=recall_for_prec_thres, color='g')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.legend(['PR curve',\n",
    "                    f'Precision {precision[i + 1]: .2f} corresponding to selected recall threshold',\n",
    "                    f'Recall {recall_for_prec_thres: .2f} corresponding to selected precision threshold'])\n",
    "        plt.title(f'Precision-Recall curve for Class {class_names[class_i]}')\n",
    "    return best_recall_threshold, best_precision_threshold\n",
    "\n",
    "\n",
    "y_pred = model.predict_generator(data_generator_val, workers=num_cores)\n",
    "y_true = data_generator_val.get_labels()\n",
    "recall_thresholds = dict()\n",
    "precision_thresholds = dict()\n",
    "for i, class_name in tqdm(enumerate(class_names)):\n",
    "    recall_thresholds[class_name], precision_thresholds[class_name] = get_threshold_for_recall(y_true, y_pred, i,\n",
    "                                                                                               plot=True)\n",
    "data_generator_test = DataGenenerator(folder_imgs=test_imgs_folder, shuffle=False)\n",
    "y_pred_test = model.predict_generator(data_generator_test, workers=num_cores)\n",
    "\n",
    "image_labels_empty = set()\n",
    "for i, (img, predictions) in enumerate(zip(os.listdir(test_imgs_folder), y_pred_test)):\n",
    "    for class_i, class_name in enumerate(class_names):\n",
    "        if predictions[class_i] < recall_thresholds[class_name]:\n",
    "            image_labels_empty.add(f'{img}_{class_name}')\n",
    "\n",
    "submission = pd.read_csv('../input/densenet201cloudy/densenet201.csv')\n",
    "submission.head()\n",
    "\n",
    "predictions_nonempty = set(submission.loc[~submission['EncodedPixels'].isnull(), 'Image_Label'].values)\n",
    "\n",
    "submission.loc[submission['Image_Label'].isin(image_labels_empty), 'EncodedPixels'] = np.nan\n",
    "submission.to_csv('submission_segmentation_and_classifier.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
